{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d07e05e-898f-4def-aeae-075d8074a258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "          Drive_Name                                        Description  \\\n",
      "0  ODE-3-120070-1F12                                                      \n",
      "1  ODE-3-120023-1F1B                                                      \n",
      "2  ODE-3-210058-104B  1.5 HP, 5.8 Amp, 110-115V AC INPUT, 230V AC Ou...   \n",
      "3    EP66-0075T3I2U5  EP66-0075T3I2U5: Constant Torque, Sensorless V...   \n",
      "4    EP66-0110T3I3U5  EP66-0110T3I3U5: Constant Torque, Sensorless V...   \n",
      "\n",
      "      Price      Stock  \n",
      "0    298.00   In stock  \n",
      "1    475.20   In stock  \n",
      "2    681.60  2-4 weeks  \n",
      "3  1,265.49   In stock  \n",
      "4  1,380.36   In stock  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Base URL of the website with pagination (assuming the page number is added at the end)\n",
    "base_url = \"https://www.driveswarehouse.com/drives?page=1&max_items=96\"\n",
    "\n",
    "# Define the columns for the DataFrame\n",
    "columns = ['Drive_Name', 'Description', 'Price', 'Stock']\n",
    "\n",
    "# Initialize an empty list to store all data frames before concatenation\n",
    "data_frames = []\n",
    "\n",
    "def drives_data():\n",
    "    # Iterate through the first 10 pages\n",
    "    for page in range(1, 11):  # Pages 1 to 10\n",
    "        # Construct the URL for each page\n",
    "        url = f'https://www.driveswarehouse.com/drives?page={page}&max_items=96'\n",
    "        \n",
    "        # Send a GET request to fetch the content of the page\n",
    "        response = requests.get(url)\n",
    "        print(response.status_code)  # Check if the request was successful\n",
    "        \n",
    "        # Parse the page content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all product containers on the page\n",
    "        drives = soup.find_all('div', attrs={'class': 'type-1 BlockBox item-holder'})\n",
    "        \n",
    "        # Initialize a list to store the current page's data\n",
    "        page_data = []\n",
    "\n",
    "        # Iterate through each product container\n",
    "        for drive in drives:\n",
    "            # Extract the product name\n",
    "            product_name = drive.find('div', attrs={'class': 'name'})\n",
    "            product_name_text = product_name.get_text(strip=True) if product_name else 'N/A'\n",
    "            \n",
    "            # Extract the product description\n",
    "            product_description = drive.find('div', attrs={'class': 'description'})\n",
    "            product_description_text = product_description.get_text(strip=True) if product_description else 'N/A'\n",
    "            \n",
    "            # Extract the product price\n",
    "            product_price = drive.find('span', attrs={'itemprop': 'price'})\n",
    "            product_price_text = product_price.get_text(strip=True) if product_price else 'N/A'\n",
    "            \n",
    "            # Extract the stock status\n",
    "            stock_status = drive.find('div', attrs={'class': 'stock'})\n",
    "            stock_status_text = stock_status.get_text(strip=True) if stock_status else 'N/A'\n",
    "            \n",
    "            # Add the row data to the list for this page\n",
    "            page_data.append([product_name_text, product_description_text, product_price_text, stock_status_text])\n",
    "\n",
    "        # Convert the list of this page's data into a DataFrame\n",
    "        page_df = pd.DataFrame(page_data, columns=columns)\n",
    "        # Append this DataFrame to the list of data frames\n",
    "        data_frames.append(page_df)\n",
    "\n",
    "# Call the function to start scraping\n",
    "drives_data()\n",
    "\n",
    "# Concatenate all data frames in the list into a single DataFrame\n",
    "product_data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(product_data.head())\n",
    "\n",
    "#save the DataFrame to a CSV file\n",
    "product_data.to_csv(r'C:\\Users\\srich\\Documents\\Innomatics Course\\Assignments\\Assignment Tasks\\Project 1\\drives_warehouse_products.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4eb3f-db0b-41cc-ab26-97ce563a7b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
